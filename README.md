# CL-RAM: Comparative Language Red-teaming Assessment Module

> **An automated framework for adversarial testing and security validation of Large Language Models (LLMs).**

![Status](https://img.shields.io/badge/Status-Research%20Prototype-blue)
![Python](https://img.shields.io/badge/Python-3.10%2B-yellow)
![Framework](https://img.shields.io/badge/Security-MITRE%20ATT%26CK-red)
![Focus](https://img.shields.io/badge/Focus-LLM%20Red%20Teaming-orange)

---

## üìã Overview

**CL-RAM** (Comprehensive LLM Risk Assessment & Mitigation) is an advanced research framework developed as part of a Master's Thesis in Cybersecurity. It provides a systematic approach to evaluating LLM security by executing structured adversarial attacks based on the **MITRE ATT&CK** framework for AI.

The tool enables researchers to assess model robustness against jailbreaks, prompt injection, and multilingual vulnerabilities across locally deployed open-weights models (Llama 3, Mistral, etc.).

### üéØ Core Mission
To automate the **Red Teaming** process and provide high-precision, human-in-the-loop validation for AI safety alignment.

---

## ‚ú® Key Features

### üõ°Ô∏è Automated Adversarial Testing
- **Attack Engine:** Executes automated attacks across **14 MITRE ATT&CK Categories** (e.g., Prompt Injection, Context Manipulation, Data Exfiltration).
- **Cross-Lingual Vectors:** Specifically tests vulnerability disparities between English and low-resource languages (Bulgarian).
- **Adaptive Strategy:** Supports multi-level attack sophistication and customizable system prompts.

### ü§ñ Consensus-Based Validation Engine
- **Ensemble Architecture:** Orchestrates **10+ calibrated LLMs** (including Llama-Guard and ShieldGemma) to evaluate target model responses.
- **Voting Logic:** Implements a weighted consensus algorithm to determine if a response is harmful, achieving **>95% detection accuracy** and minimizing false negatives.

### üñ•Ô∏è "Thesis Validator Pro" (GUI Tool)
- **Human-in-the-Loop:** A custom-built Desktop GUI (Python/Tkinter) for expert manual verification of borderline cases.
- **Audit Trail:** SQLite-backed tracking of over **8,000+ validated test cases**.
- **Visual Analytics:** Real-time dashboard for monitoring success rates and attack vectors.

### ‚öôÔ∏è Resource Optimization
- **Intelligent Memory Manager:** Dynamic loading/unloading of quantized GGUF models to enable heavy inference workloads on consumer hardware without OOM errors.

---

## üèóÔ∏è Technical Architecture

The framework is modular and built entirely in Python:

| Component | Description |
| :--- | :--- |
| **`gui3.py`** | Main graphical user interface for test configuration and real-time monitoring. |
| **`attack_executor.py`** | Orchestrates the injection of adversarial prompts into target models. |
| **`improved_validator.py`** | The core consensus engine using multiple "Judge" models to assess safety. |
| **`memory_manager.py`** | Handles VRAM optimization, context swapping, and model quantization lifecycles. |
| **`visualization_engine.py`** | Generates heatmaps, 3D plots, and statistical reports (Pandas/Matplotlib). |

---

## üìä Methodologies & Standards

The framework strictly adheres to industry standards for AI Security:
* **MITRE ATT&CK for AI:** Maps attacks to known adversary tactics.
* **OWASP Top 10 for LLM:** specifically focusing on LLM01 (Prompt Injection) and LLM02 (Insecure Output Handling).
* **NIST AI RMF:** Aligned with Risk Management Framework guidelines for measuring system trustworthiness.

---

## üì∏ Screenshots

*(Note: This is a showcase repository. The full source code remains private due to ongoing academic research).*

### 1. Thesis Validator Pro - Main Interface!
<img width="1171" height="771" alt="Screenshot 2025-09-08 162831" src="https://github.com/user-attachments/assets/f95c9c12-5ea3-4d59-8bd2-95c19e5cef4a" />

> *Custom GUI for managing thousands of adversarial tests.*
<img width="2752" height="1568" alt="Gemini_Generated_Image_yyn5q7yyn5q7yyn5" src="https://github.com/user-attachments/assets/570bc3c5-1c89-4228-adb2-8c451976b22e" />


### 2. Consensus Validation Logic
> *Real-time voting system where multiple models judge the safety of a response.*<img width="2752" height="1568" alt="Gemini_Generated_Image_jvre0mjvre0mjvre" src="https://github.com/user-attachments/assets/1f267cea-078b-430c-a15c-03e1804cb8e1" />

### 3. Visualization Output
> *Heatmap generated by the engine showing vulnerability hotspots.*
<img width="1504" height="1294" alt="Screenshot 2025-09-08 203621" src="https://github.com/user-attachments/assets/5fe281f6-a60e-4eae-9374-d79be812fb3c" />
<img width="1647" height="1291" alt="Screenshot 2025-09-08 203732" src="https://github.com/user-attachments/assets/a4dce793-e6cc-4944-9908-dd9531fb7e5c" />


---

## üë®‚Äçüî¨ Author

**Anton Z. Dimitrov**
*AI Security Engineer | Red Teaming Specialist*

Developed for Master's Thesis: *"Analysis of Vulnerabilities and a Software Framework for Large Language Models"* at Nikola Vaptsarov Naval Academy (2025).

---

## ‚ö†Ô∏è Ethical Use & Disclaimer

This tool is intended **exclusively for research and educational purposes** (Red Teaming / White Hat Security).
* All tests were conducted on locally deployed models in a controlled sandbox environment.
* No harmful content generated during testing was deployed or distributed.
* The project follows responsible disclosure practices for discovered vulnerabilities.
